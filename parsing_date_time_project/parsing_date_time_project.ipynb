{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date and Time - Parsing and Cleaning Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning points:\n",
    "* Classes\n",
    "    * date class, timedelta class, datetime class\n",
    "* Formatting and Parsing methods\n",
    "    * isoformat(), strftime(), strptime()\n",
    "* Understanding timestamp\n",
    "* Doing Arithmetic with dates and times\n",
    "    * timedelta class idea\n",
    "    * duration idea\n",
    "* Time zones\n",
    "    * **naive** datetime vs datetime with timezone vs datetime with timezone automatically from `tz` database\n",
    "    * Time zone database\n",
    "        * `tz`\n",
    "* Pandas and Time zones\n",
    "    * Handling Date Columns loaded as strings: \n",
    "        * use `parse_dates` in `read_csv`\n",
    "        * use `pd.to_datetime()` at a date column and choose a format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Class, Timedelta Class (Parsing Dates - no 'time' yet)\n",
    "* Goal 1: Working with `date class` and `timedelta class`\n",
    "* Goal 2: Doing Arithmetics with dates\n",
    "\n",
    "*The idea when ONLY working with `date` (and not with `time` yet) is to have only an object of `datetime.date` type. \n",
    "\n",
    "* Creating a date object and accessing its attributes\n",
    "        * import the `date class` from the `datetime package`\n",
    "    * `from datetime import date`\n",
    "        * Start by creating a `date` object by instantiating it from the `date` class. Put them into a list.\n",
    "            * my_date_object_as_list_of_date_objects = [date(2025, 1, 22), date(2024, 1, 1)]\n",
    "        * You can use attributes on this object (e.g.: `.year`) to access its individual components\n",
    "            * my_date_object_as_list_of_date_objects[0].year -> 2025\n",
    "        * You can also use methods on this object (e.g.: `.weekday()`) to access its individual components\n",
    "            * my_date_object_as_list_of_date_objects[0].weekday() -> 2 (refers to Wednesday because Monday is 0)\n",
    "        * You can use other methods\n",
    "            * e.g.: min(my_date_object_as_list_of_date_objects) -> 2024-01-01\n",
    "    * `from datetime import timedelta`\n",
    "        * If we add/subtract dates, you get a `timedelta` object\n",
    "            * simple way: \n",
    "                * object_d2 = date(2024, 1, 30), object_d1 = date(2024, 1, 1) \n",
    "                * time_range_object =  object_d2 - object_d1\n",
    "                * You have to access the object's component: time_range_object.days -> 29\n",
    "            * We can also start with a timedelta object\n",
    "                * my_timedelta_object = timedelta(days=29) \n",
    "                * object_d1 + my_timedelta_object -> 2024-30-01\n",
    "    * Putting Dates as Strings\n",
    "        * Use cases:\n",
    "            * _put dates as filenames to organize folders_\n",
    "            * _export the dates to excel or CSV_\n",
    "        * In both cases, the idea is to FORMAT!\n",
    "        * ISO 8601 format (`YYYY-MM-DD`) using `isoformat()` method and Other Formats using `strftime()` method on the `date` object\n",
    "            * Put the date object in a list already in ISO 8601 format:\n",
    "                * my_object_as_list_of_date_objects_iso = [date(2025, 1, 22).isoformat()) date(2024, 1, 1).isoformat()] \n",
    "            * Pass a format string of your choice:\n",
    "                * object_d1_formatted = date(2024, 01, 01).strftime(\"%Y\") -> 2024\n",
    "                * object_d1_formatted = date(2024, 01, 01).strftime(\"Year is: %Y\") -> Year is: 2024\n",
    "                * object_d1_formatted = date(2024, 01, 01).strftime(\"%Y/%m/%d\") -> 2024/01/01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The object 'date(2025, 1, 22)' has the type:  <class 'datetime.date'>\n",
      "The object from the list is printed in this way:  2025-01-22\n"
     ]
    }
   ],
   "source": [
    "# Date Class\n",
    "from datetime import date\n",
    "\n",
    "my_date_object_as_list_of_date_objects = [date(2025, 1, 22), date(2024, 1, 1)]\n",
    "\n",
    "print(\"The object 'date(2025, 1, 22)' has the type: \", type(my_date_object_as_list_of_date_objects[0]))\n",
    "\n",
    "my_date_object = date(2025, 1, 22)\n",
    "\n",
    "print(\"The object from the list is printed in this way: \", my_date_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Class, Timedelta Class (Parsing Dates - now with 'time')\n",
    "* Goal 1: Working with `datetime class` and `replace()`\n",
    "* Goal 2: Parsing string dates to datetime\n",
    "* Goal 3: Understanding `timestamp`\n",
    "\n",
    "* Creating a datetime object and accessing its attributes\n",
    "        * import the `datetime class` from the `datetime package`\n",
    "    * `from datetime import datetime`\n",
    "        * E.g.: 2024-01-01 18:30:59 (precision of 0.5 seconds or 500000 microseconds\n",
    "        * Precision is importanto for finance, for example.\n",
    "        * my_datetime_object = datetime(2024, 1, 1, 18, 30, 59, 500000)\n",
    "    * `replace()` method\n",
    "        * If you want to **replace** some of the components for new ones:\n",
    "            * E.g.: 2024-01-01 18:00:00\n",
    "            * my_datetime_object_with_new_hour = datetime(minute=0, second=0, microsecond=0)\n",
    "    * Parsing string dates date using `strftime()` and `strptime()`\n",
    "        * E.g.: `\"2024-01-01 18:30:59\"` as **string**\n",
    "            * \"2024-01-01 18:30:59\" \n",
    "            * my_datetime_object_from_string_to_datetime = datetime.strptime(\"2024-01-01 18:30:59\", \"%m/%d/%Y %H:%M:%S\")\n",
    "            * my_datetime_object_from_string_to_datetime -> 2024-01-01 18:30:59 (as datetime object)\n",
    "    * Timestamp\n",
    "        * _Computers store datetime information as the number of seconds since 1970-01-01 (when modern computers were born)_\n",
    "        * Converting from `timestamp` to `datetime`\n",
    "            * datetime.fromtimestamp(1704133859.0) -> 2024-01-01 18:30:59 (as datetime object)\n",
    "    * Duration and timedeltas\n",
    "        * If we add/subtract dates with time, you get a `timedelta` object\n",
    "        * simple way: \n",
    "            * object_d2_with_time = datetime(2024, 1, 1, 18, 30, 59), object_d1_with_time = date(2024, 1, 1, 18, 30, 9) \n",
    "            * time_range_object =  object_d2 - object_d1\n",
    "            * You have to access the object's component: time_range_object.total_seconds() -> 50\n",
    "        * We can also start with a timedelta object\n",
    "            * my_timedelta_object = timedelta(seconds=50) \n",
    "            * object_d1_with_time + my_timedelta_object -> 2024-01-01 18:30:59\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Zones\n",
    "* Goal 1: Working with `timezone class` and making `datetime` **aware** of a timezone\n",
    "* Goal 2: Handling time zones in 3 ways\n",
    "* Remember: if you want to get absolute time differences, always move to UTC!\n",
    "\n",
    "* Naive Datetime object\n",
    "    * An object can be aware or naive depending on whether it has timezone information.\n",
    "    * datetime(2024, 1, 1, 18, 30, 59) -> 2024-01-01 18:30:59\n",
    "        * There is not timezone attribute\n",
    "    * datetime(2024, 1, 1, 18, 30, 59, tzinfo=timezone(timedelta(hours=-2))) -> 2024-01-01 18:30:59-02:00\n",
    "\n",
    "* 3 Ways to Represent **non-naive** Time zones\n",
    "    * E.g.: 12:00 in UTC+2 = 10:00 in UTC \n",
    "        1) Imposing UTC+2 \n",
    "            * 12:00 in UTC+2 \n",
    "            * -> datetime(2024, 1, 1, 12, 00, 00, tzinfo = timezone(timedelta(hours=+2)))\n",
    "        2) Converting the time itself to the UTC you want \n",
    "            * 12:00 in UTC+2 = 10:00 in UTC\n",
    "            * -> datetime(2024, 1, 1, 12, 00, 00, tzinfo = timezone(timedelta(hours=+2))).astimezone(timezone.utc)\n",
    "        3) Automatic Time zones: \n",
    "            * Using the `tz` database to get the timezone you want\n",
    "            * 12:00 in UTC+2\n",
    "            * -> datetime(2024, 1, 1, 12, 00, 00, tzinfo = tz.gettz('America/New_York'))\n",
    "    * In the first case, we have the time of the clock in UTC+2\n",
    "    * In the second one, we have the same time of the clock in UTC\n",
    "    * In the third case, we have the time in the UTC we want by using the `tz` database\n",
    "    * Creating a timezone object \n",
    "        * `from datetime import timezone`\n",
    "            * Changing a `datetime` object to a specific timezone using `tzinfo` attribute \n",
    "                * Creating a timezone object with `timedelta`\n",
    "                    * timezone_object_utc_minus5 = timezone(timedelta(hours=-5)) - New York time zone\n",
    "                    * my_datetime_object_with_timezone = datetime(2024, 1, 1, 18, 30, 59, tzinfo = timezone_object_utc_minus5)\n",
    "                        * -> 2024-01-01 18:30:59-05:00\n",
    "    * Time zone Database\n",
    "        * Goal: to have *updated* time zone when it changes.\n",
    "        * A third way is to use database `tz` from `dateutil`package, as well as `gettz()`\n",
    "        * First, create a timezone object\n",
    "            * my_timezone_object = tz.gettz('America/New_York')\n",
    "        * Now, load it into the datetime object:\n",
    "            * my_datetime_object_with_timezone_from database = datetime(2024, 1, 1, 18, 30, 59, tzinfo = my_timezone_object) -> 2024-01-01 18:30:59-05:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madrid time zone object:  tzfile('Europe/Madrid')\n",
      "Naive datetime (assumed UTC): 2024-01-01 18:30:59\n",
      "Set to UTC datetime: 2024-01-01 18:30:59+00:00\n",
      "Converted to Madrid timezone: 2024-01-01 19:30:59+01:00\n"
     ]
    }
   ],
   "source": [
    "from dateutil import tz\n",
    "from datetime import datetime\n",
    "\n",
    "# Step 1: Create the timezone object for Europe/Madrid\n",
    "madrid_tz = tz.gettz('Europe/Madrid')\n",
    "print(\"Madrid time zone object: \", madrid_tz)\n",
    "\n",
    "# Step 2: Create a naive datetime object (assumed to be in UTC)\n",
    "my_naive_datetime_object = datetime(2024, 1, 1, 18, 30, 59)\n",
    "\n",
    "# Step 3: Mark the naive datetime as being in UTC\n",
    "utc_datetime_object = my_naive_datetime_object.replace(tzinfo=tz.UTC)\n",
    "\n",
    "# Step 4: Convert the UTC datetime to the Madrid timezone\n",
    "madrid_datetime_object = utc_datetime_object.astimezone(madrid_tz)\n",
    "\n",
    "# Print the results\n",
    "print(\"Naive datetime (assumed UTC):\", my_naive_datetime_object)\n",
    "print(\"Set to UTC datetime:\", utc_datetime_object)\n",
    "print(\"Converted to Madrid timezone:\", madrid_datetime_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daylight Saving Time (or Daylight \"Shifiting\" Time or Summer Time)\n",
    "* Notes:\n",
    "    * Goals of having a dayligh saving time: having longer summer evenings  \n",
    "    * Clocks moving forward in the Spring\n",
    "    * Clocks moving back in the Fall\n",
    "* Goal 1: Handling time zones with Dayligh Savings\n",
    "    * Doing it manually\n",
    "    * Doing it automatically with the Time Zone Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clocks moving forward in the Spring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive datetime (before daylight savings): 2024-01-01T07:59:59\n",
      "Datetime object (after daylight savings): 2024-01-01T09:00:00\n",
      "Type of the datetime object: <class 'datetime.datetime'>\n",
      "Time difference between the two datetimes: 1:00:01\n",
      "Type of the time difference object: <class 'datetime.timedelta'>\n",
      "Time difference in seconds: 3601.0 (1 hour and 1 second apart)\n",
      "Datetime object (before daylight savings): 2024-01-01T07:59:59+00:00\n",
      "Datetime object (after daylight savings): 2024-01-01T09:00:00+01:00\n",
      "Type of the datetime object with timezone information: <class 'datetime.datetime'>\n",
      "Time difference between the two datetimes: 0:00:01\n",
      "Type of the time difference object: <class 'datetime.timedelta'>\n"
     ]
    }
   ],
   "source": [
    "from datetime import timezone, timedelta\n",
    "\n",
    "# Step 1: Create a naive datetime object to represent '2024-01-01 07:59:59', before daylight savings starts\n",
    "my_naive_datetime_object_before_daylight_savings = datetime(2024, 1, 1, 7, 59, 59)\n",
    "print(\"Naive datetime (before daylight savings):\", my_naive_datetime_object_before_daylight_savings.isoformat()) \n",
    "\n",
    "# Step 2: Create another datetime object to represent '2024-01-01 09:00:00', 1 min after daylight savings starts\n",
    "# Note: 08:00:00 is skipped on that day due to daylight savings\n",
    "my_datetime_object_after_daylight_savings = datetime(2024, 1, 1, 9, 0, 0)\n",
    "print(\"Datetime object (after daylight savings):\", my_datetime_object_after_daylight_savings.isoformat())\n",
    "print(\"Type of the datetime object:\", type(my_datetime_object_after_daylight_savings))\n",
    "\n",
    "# Step 3: Calculate the difference between the two datetimes with the timedelta object\n",
    "time_difference = my_datetime_object_after_daylight_savings - my_naive_datetime_object_before_daylight_savings\n",
    "print(\"Time difference between the two datetimes:\", time_difference)\n",
    "print(\"Type of the time difference object:\", type(time_difference))\n",
    "\n",
    "# Apply the total_seconds() method to the timedelta object\n",
    "time_difference_in_seconds = time_difference.total_seconds()\n",
    "\n",
    "# print the result\n",
    "print(\"Time difference in seconds:\", time_difference_in_seconds, \"(1 hour and 1 second apart)\")\n",
    "\n",
    "# Now, let's do the same as above, but with the timezone class.\n",
    "\n",
    "# Step 1: Create a timezone object for the Madrid timezone and for the UTC timezone\n",
    "# Let's assume that the naive datetime object is is in UTC and the other one in the Madrid timezone.\n",
    "timezone_object_in_utc = timezone(timedelta(hours=0))\n",
    "timezone_object_in_utc1 = timezone(timedelta(hours=1))\n",
    "\n",
    "# Step 2: Use the replace() method to set the timezone for the naive datetime object (before daylight savings)\n",
    "# and the other datetime object (after daylight savings)\n",
    "my_datetime_object_before_daylight_savings_w_tz = my_naive_datetime_object_before_daylight_savings.replace(tzinfo=timezone_object_in_utc)\n",
    "my_datetime_object_after_daylight_savings_w_tz = my_datetime_object_after_daylight_savings.replace(tzinfo=timezone_object_in_utc1)\n",
    "print(\"Datetime object (before daylight savings):\", my_datetime_object_before_daylight_savings_w_tz.isoformat())\n",
    "print(\"Datetime object (after daylight savings):\", my_datetime_object_after_daylight_savings_w_tz.isoformat())\n",
    "# print the type of the datetime object with timezone information\n",
    "print(\"Type of the datetime object with timezone information:\", type(my_datetime_object_after_daylight_savings_w_tz))\n",
    "# create a timedelta object to represent the time difference between the two datetimes\n",
    "time_difference_w_tz = my_datetime_object_after_daylight_savings_w_tz - my_datetime_object_before_daylight_savings_w_tz\n",
    "print(\"Time difference between the two datetimes:\", time_difference_w_tz)\n",
    "print(\"Type of the time difference object:\", type(time_difference_w_tz))\n",
    "# apply the total_seconds() method to the timedelta object\n",
    "time_difference_in_seconds_w_tz = time_difference_w_tz.total_seconds()\n",
    "# Note that now the time difference is 1 second and not 1 hour and 1 second, this is because\n",
    "# the timezone information is taken into account and the time difference is calculated in the same timezone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madrid time zone object: tzfile('Europe/Madrid')\n",
      "Datetime object (before daylight savings): 2024-01-01T07:59:59+01:00\n",
      "Datetime object (after daylight savings): 2024-01-01T09:00:00+01:00\n",
      "Time difference between the two datetimes: 1:00:01\n",
      "Type of the time difference object: <class 'datetime.timedelta'>\n"
     ]
    }
   ],
   "source": [
    "# Now, let's use the Time Zone Database (tz) to get the timezone information for the Madrid timezone.\n",
    "from dateutil import tz\n",
    "from datetime import datetime\n",
    "\n",
    "# Step 1: Create the timezone object for Europe/Madrid\n",
    "madrid_tz = tz.gettz('Europe/Madrid')\n",
    "print(\"Madrid time zone object:\", madrid_tz)\n",
    "\n",
    "# assign the timezone object to the datetime object\n",
    "my_datetime_object_before_daylight_savings_w_tz_db = datetime(2024, 1, 1, 7, 59, 59, tzinfo=madrid_tz)\n",
    "my_datetime_object_after_daylight_savings_w_tz_db = datetime(2024, 1, 1, 9, 0, 0, tzinfo=madrid_tz)\n",
    "\n",
    "# Note that the tzinfo attribute is set to the timezone object for the Madrid timezone and it figures out the daylight savings for us.\n",
    "\n",
    "# print the datetime objects\n",
    "print(\"Datetime object (before daylight savings):\", my_datetime_object_before_daylight_savings_w_tz_db.isoformat())\n",
    "print(\"Datetime object (after daylight savings):\", my_datetime_object_after_daylight_savings_w_tz_db.isoformat())\n",
    "\n",
    "# create a timedelta object to represent the time difference between the two datetimes\n",
    "time_difference_w_tz_db = my_datetime_object_after_daylight_savings_w_tz_db - my_datetime_object_before_daylight_savings_w_tz_db\n",
    "print(\"Time difference between the two datetimes:\", time_difference_w_tz_db)\n",
    "print(\"Type of the time difference object:\", type(time_difference_w_tz_db))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Start: 2017-03-12 00:00:00\n",
      "Naive End: 2017-03-12 06:00:00\n",
      "Time Zone-Aware Start: 2017-03-12T00:00:00-05:00\n",
      "Time Zone-Aware End: 2017-03-12T06:00:00-04:00\n",
      "Time Difference (Local Time): 6.0 hours\n",
      "Start in UTC: 2017-03-12T05:00:00+00:00\n",
      "End in UTC: 2017-03-12T10:00:00+00:00\n",
      "Time Difference (UTC): 5.0 hours\n",
      "DST Start: 2017-03-12T01:30:00-05:00\n",
      "DST End: 2017-03-12T02:30:00-04:00\n",
      "Time Difference (Local Time): 1.0 hours\n",
      "Time Difference (UTC): 0.0 hours\n"
     ]
    }
   ],
   "source": [
    "# Now, let's put it altogather and show why if you want to get absolute time differences, you should always move to UTC.\n",
    "\n",
    "# Step 1: Naive Datetime Objects\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Create naive datetime objects (no time zone information)\n",
    "start_naive = datetime(2017, 3, 12, 0, 0)  # March 12, 2017, midnight\n",
    "end_naive = start_naive + timedelta(hours=6)  # Add 6 hours\n",
    "\n",
    "print(\"Naive Start:\", start_naive)\n",
    "print(\"Naive End:\", end_naive)\n",
    "\n",
    "# Step 2: Add Time Zone Information\n",
    "\n",
    "from dateutil import tz\n",
    "\n",
    "# Make the datetime objects time zone-aware\n",
    "start_aware = start_naive.replace(tzinfo=tz.gettz('America/New_York'))\n",
    "end_aware = end_naive.replace(tzinfo=tz.gettz('America/New_York'))\n",
    "\n",
    "print(\"Time Zone-Aware Start:\", start_aware.isoformat())\n",
    "print(\"Time Zone-Aware End:\", end_aware.isoformat())\n",
    "\n",
    "# Step 3: Calculate Time Difference in Local Time\n",
    "# Calculate the time difference in hours\n",
    "time_diff_hours = (end_aware - start_aware).total_seconds() / (60 * 60)\n",
    "print(\"Time Difference (Local Time):\", time_diff_hours, \"hours\")\n",
    "\n",
    "# Step 4: Convert to UTC for Absolute Time Differences\n",
    "from datetime import timezone\n",
    "\n",
    "# Convert to UTC\n",
    "start_utc = start_aware.astimezone(timezone.utc)\n",
    "end_utc = end_aware.astimezone(timezone.utc)\n",
    "\n",
    "print(\"Start in UTC:\", start_utc.isoformat())\n",
    "print(\"End in UTC:\", end_utc.isoformat())\n",
    "\n",
    "# Calculate the time difference in UTC\n",
    "time_diff_hours_utc = (end_utc - start_utc).total_seconds() / (60 * 60)\n",
    "print(\"Time Difference (UTC):\", time_diff_hours_utc, \"hours\")\n",
    "\n",
    "# Step 5: Why UTC Matters\n",
    "# Example with DST transition\n",
    "start_dst = datetime(2017, 3, 12, 1, 30, tzinfo=tz.gettz('America/New_York'))  # 1:30 AM\n",
    "end_dst = start_dst + timedelta(hours=1)  # Add 1 hour\n",
    "\n",
    "print(\"DST Start:\", start_dst.isoformat())\n",
    "print(\"DST End:\", end_dst.isoformat())\n",
    "\n",
    "# Calculate local time difference\n",
    "time_diff_hours_dst = (end_dst - start_dst).total_seconds() / (60 * 60)\n",
    "print(\"Time Difference (Local Time):\", time_diff_hours_dst, \"hours\")\n",
    "\n",
    "# Convert to UTC and calculate difference\n",
    "start_dst_utc = start_dst.astimezone(timezone.utc)\n",
    "end_dst_utc = end_dst.astimezone(timezone.utc)\n",
    "time_diff_hours_dst_utc = (end_dst_utc - start_dst_utc).total_seconds() / (60 * 60)\n",
    "print(\"Time Difference (UTC):\", time_diff_hours_dst_utc, \"hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000-03-29T00:00:00+01:00\n",
      "2001-03-29T00:00:00+01:00\n",
      "2002-03-29T00:00:00+00:00\n",
      "2003-03-29T00:00:00+00:00\n",
      "2004-03-29T00:00:00+01:00\n",
      "2005-03-29T00:00:00+01:00\n",
      "2006-03-29T00:00:00+01:00\n",
      "2007-03-29T00:00:00+01:00\n",
      "2008-03-29T00:00:00+00:00\n",
      "2009-03-29T00:00:00+00:00\n",
      "2010-03-29T00:00:00+01:00\n"
     ]
    }
   ],
   "source": [
    "# Now, notice how it is important to use the tz database to get the correct time zone information, as shown in the following example.\n",
    "# The DST (Daylight Saving Time) might change according to the year, and the tz database has this information.\n",
    "\n",
    "# Import datetime and tz\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "\n",
    "# Create starting date\n",
    "dt = datetime(2000, 3, 29, tzinfo = tz.gettz('Europe/London'))\n",
    "\n",
    "# Loop over the dates, replacing the year, and print the ISO timestamp\n",
    "for y in range(2000, 2011):\n",
    "  print(dt.replace(year=y).isoformat())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clocks moving back in the Fall\n",
    "\n",
    "* Daylight Saving Time (DST) means that, at some point in the year, the clocks move forward (spring forward) or backward (fall back).\n",
    "    * When the clock moves forward, an hour is \"skipped.\"\n",
    "    * When the clock moves backward, the same hour happens twice, which can cause confusion when handling datetime values.\n",
    "\n",
    "* Problem: When clocks \"fall back,\" some times occur twice, causing ambiguous datetime values.\n",
    "* Solution: Use tz.enfold() to distinguish between two instances of the same local time.\n",
    "* Best Practice: Convert to UTC before performing datetime arithmetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is incorrect: -1 day, 23:45:00\n",
      "{'start': datetime.datetime(2023, 11, 4, 23, 30, tzinfo=zoneinfo.ZoneInfo(key='America/New_York')), 'end': datetime.datetime(2023, 11, 5, 0, 15, tzinfo=zoneinfo.ZoneInfo(key='America/New_York'))}\n",
      "Shortest trip: 1800.0\n"
     ]
    }
   ],
   "source": [
    "# Step 1: The Problem with Naive Datetime Arithmetic\n",
    "# Suppose we have a bike trip that starts at 1:30 AM and ends at 1:15 AM on a day when\n",
    "# the clocks move backward at 2:00 AM → 1:00 AM (fall back).\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "start = datetime(2023, 11, 5, 1, 30, tzinfo=ZoneInfo(\"America/New_York\"))\n",
    "end = datetime(2023, 11, 5, 1, 15, tzinfo=ZoneInfo(\"America/New_York\"))\n",
    "\n",
    "# If we subtract these two timestamps without handling DST, Python may think that end < start and return a negative duration.\n",
    "print(\"This is incorrect:\", end - start)  # Incorrect!\n",
    "\n",
    "# Step 2: Using tz.enfold() to Handle the Ambiguous Time\n",
    "# Python’s tz.enfold() (from dateutil.tz) helps resolve the ambiguity by marking which \n",
    "# instance of 1:15 AM we are referring to.\n",
    "\n",
    "from dateutil import tz\n",
    "\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo  # Use tzfile for proper timezone handling\n",
    "\n",
    "onebike_datetimes = [\n",
    "    {\n",
    "        \"start\": datetime(2023, 11, 4, 23, 30, tzinfo=ZoneInfo(\"America/New_York\")),\n",
    "        \"end\": datetime(2023, 11, 5, 0, 15, tzinfo=ZoneInfo(\"America/New_York\"))\n",
    "    },\n",
    "    {\n",
    "        \"start\": datetime(2023, 11, 5, 0, 45, tzinfo=ZoneInfo(\"America/New_York\")),\n",
    "        \"end\": datetime(2023, 11, 5, 1, 30, tzinfo=ZoneInfo(\"America/New_York\"))\n",
    "    },\n",
    "    {\n",
    "        \"start\": datetime(2023, 11, 5, 1, 15, tzinfo=ZoneInfo(\"America/New_York\")),  \n",
    "        \"end\": datetime(2023, 11, 5, 1, 45, tzinfo=ZoneInfo(\"America/New_York\"))  # Ambiguous time (DST rollback)\n",
    "    },\n",
    "    {\n",
    "        \"start\": datetime(2023, 11, 5, 1, 30, tzinfo=ZoneInfo(\"America/New_York\")),  \n",
    "        \"end\": datetime(2023, 11, 5, 2, 15, tzinfo=ZoneInfo(\"America/New_York\"))\n",
    "    },\n",
    "    {\n",
    "        \"start\": datetime(2023, 11, 5, 2, 30, tzinfo=ZoneInfo(\"America/New_York\")),\n",
    "        \"end\": datetime(2023, 11, 5, 3, 15, tzinfo=ZoneInfo(\"America/New_York\"))\n",
    "    }\n",
    "]\n",
    "\n",
    "# Print first entry to verify\n",
    "print(onebike_datetimes[0])\n",
    "\n",
    "\n",
    "trip_durations = []\n",
    "for trip in onebike_datetimes:\n",
    "    # Check if the trip crosses a DST transition where the clock goes back\n",
    "    if trip[\"start\"] > trip[\"end\"]:\n",
    "        trip[\"end\"] = tz.enfold(trip[\"end\"])  # Disambiguate the end time\n",
    "\n",
    "    # Convert times to UTC to ensure consistency\n",
    "    start = trip[\"start\"].astimezone(tz.UTC)\n",
    "    end = trip[\"end\"].astimezone(tz.UTC)\n",
    "\n",
    "    # Compute trip duration in seconds\n",
    "    trip_length_seconds = (end - start).total_seconds()\n",
    "    trip_durations.append(trip_length_seconds)\n",
    "\n",
    "# Step 3: Converting to UTC to Avoid Ambiguities\n",
    "start = trip[\"start\"].astimezone(tz.UTC)\n",
    "end = trip[\"end\"].astimezone(tz.UTC)\n",
    "\n",
    "# Step 4: Computing the Trip Duration\n",
    "trip_length_seconds = (end - start).total_seconds()\n",
    "trip_durations.append(trip_length_seconds)\n",
    "\n",
    "# Step 5: Finding the Shortest Trip\n",
    "print(\"Shortest trip: \" + str(min(trip_durations)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas and Time zones\n",
    "\n",
    "* Goal 1: use `parse_dates` in `read_csv`\n",
    "    * columns with dates as strings will become `datetime` objects\n",
    "* Goal 2: use `pd.to_datetime()` at a date column and choose a format\n",
    "* Goal 3: applying math methods will return `timedelta` objects\n",
    "* Goal 4: using `timezone` in Pandas\n",
    "    * Handle Daylight Savings (DST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'C:/Users/caiov/OneDrive - UCLA IT Services/Documentos/DataScience/Repositories/cleaning-parsing-date-time-best-practices/data/bike_rides.csv' created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Let's create a CSV file with the bike rides data using the csv module in Python\n",
    "import csv\n",
    "\n",
    "# Define the correct file path with the file name\n",
    "csv_filename = \"C:/Users/caiov/OneDrive - UCLA IT Services/Documentos/DataScience/Repositories/cleaning-parsing-date-time-best-practices/data/bike_rides.csv\"\n",
    "\n",
    "# Sample ride data\n",
    "rides = [\n",
    "    {\"ride_id\": 1, \"start_date\": \"2017-10-01 15:23:25\", \"end_date\": \"2017-10-01 15:26:26\"},\n",
    "    {\"ride_id\": 2, \"start_date\": \"2017-10-02 08:12:45\", \"end_date\": \"2017-10-02 08:25:13\"},\n",
    "    {\"ride_id\": 3, \"start_date\": \"2017-10-03 21:30:10\", \"end_date\": \"2017-10-03 21:45:55\"},\n",
    "    {\"ride_id\": 4, \"start_date\": \"2017-10-04 06:50:33\", \"end_date\": \"2017-10-04 07:05:12\"},\n",
    "    {\"ride_id\": 5, \"start_date\": \"2017-10-05 12:10:00\", \"end_date\": \"2017-10-05 12:40:45\"}\n",
    "]\n",
    "\n",
    "# Try writing the file\n",
    "try:\n",
    "    with open(csv_filename, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"ride_id\", \"start_date\", \"end_date\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rides)\n",
    "    print(f\"CSV file '{csv_filename}' created successfully!\")\n",
    "\n",
    "except PermissionError:\n",
    "    print(f\"Permission denied: Unable to write to '{csv_filename}'.\\nTry saving in a different folder or checking file permissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/caiov/OneDrive - UCLA IT Services/Documentos/DataScience/Repositories/cleaning-parsing-date-time-best-practices/data/bike_rides.csv'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ride_id                         1\n",
      "start_date    2017-10-01 15:23:25\n",
      "end_date      2017-10-01 15:26:26\n",
      "Name: 0, dtype: object\n",
      "<class 'pandas._libs.tslibs.timestamps.Timestamp'>\n",
      "The 'start_date' column has a datetime type:  datetime64[ns]\n",
      "0     181.0\n",
      "1     748.0\n",
      "2     945.0\n",
      "3     879.0\n",
      "4    1845.0\n",
      "Name: duration, dtype: float64\n",
      "The mean of the duration column is:  919.6\n",
      "The type of the duration_as_timedelta column with the mean is:  <class 'pandas._libs.tslibs.timedeltas.Timedelta'>\n"
     ]
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV into the rides variable\n",
    "rides = pd.read_csv(csv_filename, parse_dates = ['start_date', 'end_date'])\n",
    "\n",
    "rides_copy = rides.copy()\n",
    "\n",
    "\n",
    "# Print the initial (0th) row\n",
    "print(rides.iloc[0])\n",
    "\n",
    "# Print the data type of the rides.iloc[0]['start_date'] column\n",
    "print(type(rides.iloc[0]['start_date']))\n",
    "\n",
    "# Print the data type of the 'start_date' column\n",
    "print(\"The 'start_date' column has a datetime type: \", rides['start_date'].dtype)\n",
    "\n",
    "# Create a duration column as timedelta\n",
    "rides['duration_as_timedelta'] = rides['end_date'] - rides['start_date']\n",
    "\n",
    "# Subtract the start_date from the end date and Convert to seconds\n",
    "rides['duration'] = (rides['end_date'] - rides['start_date']).dt.total_seconds()\n",
    "\n",
    "# Print the first few rows of the duration column\n",
    "print(rides['duration'].head())\n",
    "\n",
    "# Print the mean of the duration column\n",
    "print(\"The mean of the duration column is: \", rides['duration'].mean())\n",
    "\n",
    "# Print the type of the duration_as_timedelta column with the mean\n",
    "print(\"The type of the duration_as_timedelta column with the mean is: \", type(rides['duration_as_timedelta'].mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-01 15:23:25-04:00\n",
      "2017-10-01 20:23:25+01:00\n"
     ]
    }
   ],
   "source": [
    "# Dates start as naive datetime objects, we want to normalize them in a timezone\n",
    "# tz_localize() method to localize the naive datetime objects\n",
    "# if we try to convert all the column to a timezone, there will be an ambiguity error becasue of the Daylight Saving Time\n",
    "\n",
    "# Localize the start_date column to America/New_York\n",
    "rides_copy['start_date'] = rides_copy['start_date'].dt.tz_localize('America/New_York', ambiguous='NaT')\n",
    "\n",
    "# Print first value\n",
    "print(rides_copy['start_date'].iloc[0])\n",
    "\n",
    "# Convert the start_date column to Europe/London\n",
    "rides_copy['start_date'] = rides_copy['start_date'].dt.tz_convert('Europe/London')\n",
    "\n",
    "# Print the new value\n",
    "print(rides_copy['start_date'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our dataset\n",
    "\n",
    "| Column                   | Description                                                                      |\n",
    "|------------------------- |--------------------------------------------------------------------------------- |\n",
    "| `student_id`             | A unique ID for each student.                                                    |\n",
    "| `city`                   | A code for the city the student lives in.                                        |\n",
    "| `city_development_index` | A scaled development index for the city.                                         |\n",
    "| `gender`                 | The student's gender.                                                            |\n",
    "| `relevant_experience`    | An indicator of the student's work relevant experience.                          |\n",
    "| `enrolled_university`    | The type of university course enrolled in (if any).                              |\n",
    "| `education_level`        | The student's education level.                                                   |\n",
    "| `major_discipline`       | The educational discipline of the student.                                       |\n",
    "| `experience`             | The student's total work experience (in years).                                  |\n",
    "| `company_size`           | The number of employees at the student's current employer.                       |\n",
    "| `company_type`           | The type of company employing the student.                                       |\n",
    "| `last_new_job`           | The number of years between the student's current and previous jobs.             |\n",
    "| `training_hours`         | The number of hours of training completed.                                       |\n",
    "| `job_change`             | An indicator of whether the student is looking for a new job (`1`) or not (`0`). |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>city</th>\n",
       "      <th>city_development_index</th>\n",
       "      <th>gender</th>\n",
       "      <th>relevant_experience</th>\n",
       "      <th>enrolled_university</th>\n",
       "      <th>education_level</th>\n",
       "      <th>major_discipline</th>\n",
       "      <th>experience</th>\n",
       "      <th>company_size</th>\n",
       "      <th>company_type</th>\n",
       "      <th>last_new_job</th>\n",
       "      <th>training_hours</th>\n",
       "      <th>job_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8949</td>\n",
       "      <td>city_103</td>\n",
       "      <td>0.920</td>\n",
       "      <td>Male</td>\n",
       "      <td>Has relevant experience</td>\n",
       "      <td>no_enrollment</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>STEM</td>\n",
       "      <td>&gt;20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29725</td>\n",
       "      <td>city_40</td>\n",
       "      <td>0.776</td>\n",
       "      <td>Male</td>\n",
       "      <td>No relevant experience</td>\n",
       "      <td>no_enrollment</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>STEM</td>\n",
       "      <td>15</td>\n",
       "      <td>50-99</td>\n",
       "      <td>Pvt Ltd</td>\n",
       "      <td>&gt;4</td>\n",
       "      <td>47</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11561</td>\n",
       "      <td>city_21</td>\n",
       "      <td>0.624</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No relevant experience</td>\n",
       "      <td>Full time course</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>STEM</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>never</td>\n",
       "      <td>83</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33241</td>\n",
       "      <td>city_115</td>\n",
       "      <td>0.789</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No relevant experience</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Business Degree</td>\n",
       "      <td>&lt;1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pvt Ltd</td>\n",
       "      <td>never</td>\n",
       "      <td>52</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>666</td>\n",
       "      <td>city_162</td>\n",
       "      <td>0.767</td>\n",
       "      <td>Male</td>\n",
       "      <td>Has relevant experience</td>\n",
       "      <td>no_enrollment</td>\n",
       "      <td>Masters</td>\n",
       "      <td>STEM</td>\n",
       "      <td>&gt;20</td>\n",
       "      <td>50-99</td>\n",
       "      <td>Funded Startup</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   student_id      city  city_development_index gender  \\\n",
       "0        8949  city_103                   0.920   Male   \n",
       "1       29725   city_40                   0.776   Male   \n",
       "2       11561   city_21                   0.624    NaN   \n",
       "3       33241  city_115                   0.789    NaN   \n",
       "4         666  city_162                   0.767   Male   \n",
       "\n",
       "       relevant_experience enrolled_university education_level  \\\n",
       "0  Has relevant experience       no_enrollment        Graduate   \n",
       "1   No relevant experience       no_enrollment        Graduate   \n",
       "2   No relevant experience    Full time course        Graduate   \n",
       "3   No relevant experience                 NaN        Graduate   \n",
       "4  Has relevant experience       no_enrollment         Masters   \n",
       "\n",
       "  major_discipline experience company_size    company_type last_new_job  \\\n",
       "0             STEM        >20          NaN             NaN            1   \n",
       "1             STEM         15        50-99         Pvt Ltd           >4   \n",
       "2             STEM          5          NaN             NaN        never   \n",
       "3  Business Degree         <1          NaN         Pvt Ltd        never   \n",
       "4             STEM        >20        50-99  Funded Startup            4   \n",
       "\n",
       "   training_hours  job_change  \n",
       "0              36         1.0  \n",
       "1              47         0.0  \n",
       "2              83         0.0  \n",
       "3              52         1.0  \n",
       "4               8         0.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "ds_jobs = pd.read_csv(\"C:/Users/caiov/OneDrive - UCLA IT Services/Documentos/DataScience/Repositories/cleaning-categorical-data-best-practices/data/customer_not_efficient.csv\")\n",
    "\n",
    "# View the dataset\n",
    "ds_jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of ds_jobs for transforming\n",
    "ds_jobs_transformed = ds_jobs.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19158 entries, 0 to 19157\n",
      "Data columns (total 14 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   student_id              19158 non-null  int64  \n",
      " 1   city                    19158 non-null  object \n",
      " 2   city_development_index  19158 non-null  float64\n",
      " 3   gender                  14650 non-null  object \n",
      " 4   relevant_experience     19158 non-null  object \n",
      " 5   enrolled_university     18772 non-null  object \n",
      " 6   education_level         18698 non-null  object \n",
      " 7   major_discipline        16345 non-null  object \n",
      " 8   experience              19093 non-null  object \n",
      " 9   company_size            13220 non-null  object \n",
      " 10  company_type            13018 non-null  object \n",
      " 11  last_new_job            18735 non-null  object \n",
      " 12  training_hours          19158 non-null  int64  \n",
      " 13  job_change              19158 non-null  float64\n",
      "dtypes: float64(2), int64(2), object(10)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "ds_jobs_transformed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Numeric Columns\n",
    "# Note: job_change should be a categorical column and not a numeric column\n",
    "\n",
    "numeric_columns = ['student_id', 'city_development_index', 'training_hours']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Procedures\n",
    "\n",
    "| **Integer Columns**               | **Float Columns**             |\n",
    "|-----------------------------------|-------------------------------|\n",
    "| Store as 32-bit integers (`int32`) | Store as 16-bit floats (`float16`) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "student_id                  int32\n",
      "city_development_index    float16\n",
      "training_hours              int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert the numeric columns according to the table above\n",
    "\n",
    "for col in ds_jobs_transformed[numeric_columns]:\n",
    "    if pd.api.types.is_integer_dtype(ds_jobs_transformed[col]):\n",
    "        ds_jobs_transformed[col] = ds_jobs_transformed[col].astype('int32')\n",
    "    elif pd.api.types.is_float_dtype(ds_jobs_transformed[col]):\n",
    "        ds_jobs_transformed[col] = ds_jobs_transformed[col].astype('float16')\n",
    "\n",
    "print(ds_jobs_transformed[numeric_columns].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['city', 'gender', 'relevant_experience', 'enrolled_university', 'education_level', 'major_discipline', 'experience', 'company_size', 'company_type', 'last_new_job', 'job_change']\n"
     ]
    }
   ],
   "source": [
    "# List of Categorical Columns\n",
    "categorical_columns = list(ds_jobs_transformed.select_dtypes(include=['object', 'category']).columns)\n",
    "\n",
    "# Including `job_change` in the list of categorical columns\n",
    "categorical_columns = categorical_columns + ['job_change']\n",
    "\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "city                   123\n",
       "gender                   3\n",
       "relevant_experience      2\n",
       "enrolled_university      3\n",
       "education_level          5\n",
       "major_discipline         6\n",
       "experience              22\n",
       "company_size             8\n",
       "company_type             6\n",
       "last_new_job             6\n",
       "job_change               2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_jobs_transformed[categorical_columns].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating Categorical Columns by its nature\n",
    "ls_categorical_bool = ['relevant_experience', 'job_change']\n",
    "ls_categorical_with_order = ['enrolled_university', 'education_level', 'experience', 'company_size', 'last_new_job']\n",
    "ls_categorical_no_order = ['city', 'gender', 'major_discipline', 'company_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Procedures\n",
    "\n",
    "| **Converting Categorical Data**                                                                                  |\n",
    "|------------------------------------------------------------------------------------------------------------------|\n",
    "| (Two-factor categories) Data w/ **2 categories**: yes/no → Convert to `bool`                                     |\n",
    "| (Ordinal Data) Data w/ **> 2 categories** and **natural ordering** → Convert to `ordered category`               |\n",
    "| (Nominal data) Data w/ **few unique values** and **no natural ordering** → Convert to `category`                 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant_experience:  ['Has relevant experience' 'No relevant experience']\n",
      "job_change:  [1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Two-factor Categories (mapping to boolean)\n",
    "print(\"relevant_experience: \", ds_jobs_transformed['relevant_experience'].unique())\n",
    "print(\"job_change: \", ds_jobs_transformed['job_change'].unique())\n",
    "\n",
    "ds_jobs_transformed['relevant_experience'] = ds_jobs_transformed['relevant_experience'].map({'Has relevant experience': True, 'No relevant experience': False})\n",
    "ds_jobs_transformed['job_change'] = ds_jobs_transformed['job_change'].map({1: True, 0: False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant_experience:  [ True False]\n",
      "job_change:  [ True False]\n"
     ]
    }
   ],
   "source": [
    "print(\"relevant_experience: \", ds_jobs_transformed['relevant_experience'].unique())\n",
    "print(\"job_change: \", ds_jobs_transformed['job_change'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enrolled_university:  ['no_enrollment' 'Full time course' nan 'Part time course']\n",
      "enrolled_university:  0           no_enrollment\n",
      "1           no_enrollment\n",
      "2        Full time course\n",
      "3                     NaN\n",
      "4           no_enrollment\n",
      "               ...       \n",
      "19153       no_enrollment\n",
      "19154       no_enrollment\n",
      "19155       no_enrollment\n",
      "19156       no_enrollment\n",
      "19157       no_enrollment\n",
      "Name: enrolled_university, Length: 19158, dtype: category\n",
      "Categories (3, object): ['no_enrollment' < 'Part time course' < 'Full time course']\n"
     ]
    }
   ],
   "source": [
    "# Ordinal Data (converting to \"ordered category\")\n",
    "\n",
    "# enrolled_university\n",
    "print(\"enrolled_university: \", ds_jobs_transformed['enrolled_university'].unique())\n",
    "\n",
    "ls_enrolled_university_order = ['no_enrollment', 'Part time course', 'Full time course']\n",
    "\n",
    "ds_jobs_transformed['enrolled_university'] = pd.Categorical(ds_jobs_transformed['enrolled_university'], categories=ls_enrolled_university_order, ordered=True)\n",
    "\n",
    "print(\"enrolled_university: \", ds_jobs_transformed['enrolled_university'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "education_level:  ['Graduate' 'Masters' 'High School' nan 'Phd' 'Primary School']\n",
      "education_level:  0              Graduate\n",
      "1              Graduate\n",
      "2              Graduate\n",
      "3              Graduate\n",
      "4               Masters\n",
      "              ...      \n",
      "19153          Graduate\n",
      "19154          Graduate\n",
      "19155          Graduate\n",
      "19156       High School\n",
      "19157    Primary School\n",
      "Name: education_level, Length: 19158, dtype: category\n",
      "Categories (5, object): ['Primary School' < 'High School' < 'Graduate' < 'Masters' < 'Phd']\n"
     ]
    }
   ],
   "source": [
    "# education_level\n",
    "\n",
    "print(\"education_level: \", ds_jobs_transformed['education_level'].unique())\n",
    "\n",
    "ls_education_level_order = ['Primary School', 'High School', 'Graduate', 'Masters', 'Phd']\n",
    "\n",
    "ds_jobs_transformed['education_level'] = pd.Categorical(ds_jobs_transformed['education_level'], categories=ls_education_level_order, ordered=True)\n",
    "\n",
    "print(\"education_level: \", ds_jobs_transformed['education_level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experience:  ['>20' '15' '5' '<1' '11' '13' '7' '17' '2' '16' '1' '4' '10' '14' '18'\n",
      " '19' '12' '3' '6' '9' '8' '20' nan]\n",
      "experience:  0        >20\n",
      "1         15\n",
      "2          5\n",
      "3         <1\n",
      "4        >20\n",
      "        ... \n",
      "19153     14\n",
      "19154     14\n",
      "19155    >20\n",
      "19156     <1\n",
      "19157      2\n",
      "Name: experience, Length: 19158, dtype: category\n",
      "Categories (22, object): ['<1' < '1' < '2' < '3' ... '18' < '19' < '20' < '>20']\n"
     ]
    }
   ],
   "source": [
    "# experience\n",
    "print(\"experience: \", ds_jobs_transformed['experience'].unique())\n",
    "\n",
    "ls_experience_order = ['<1', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '>20']\n",
    "\n",
    "ds_jobs_transformed['experience'] = pd.Categorical(ds_jobs_transformed['experience'], categories=ls_experience_order, ordered=True)\n",
    "\n",
    "print(\"experience: \", ds_jobs_transformed['experience'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company_size:  [nan '50-99' '<10' '10000+' '5000-9999' '1000-4999' '10-49' '100-499'\n",
      " '500-999']\n",
      "company_size:  0            NaN\n",
      "1          50-99\n",
      "2            NaN\n",
      "3            NaN\n",
      "4          50-99\n",
      "          ...   \n",
      "19153        NaN\n",
      "19154        NaN\n",
      "19155      50-99\n",
      "19156    500-999\n",
      "19157        NaN\n",
      "Name: company_size, Length: 19158, dtype: category\n",
      "Categories (8, object): ['<10' < '10/49' < '50-99' < '100-500' < '500-999' < '1000-4999' < '5000-9999' < '10000+']\n"
     ]
    }
   ],
   "source": [
    "# company_size\n",
    "print(\"company_size: \", ds_jobs_transformed['company_size'].unique())\n",
    "\n",
    "ls_company_size_order = ['<10', '10/49', '50-99', '100-500', '500-999', '1000-4999', '5000-9999', '10000+']\n",
    "\n",
    "ds_jobs_transformed['company_size'] = pd.Categorical(ds_jobs_transformed['company_size'], categories=ls_company_size_order, ordered=True)\n",
    "\n",
    "print(\"company_size: \", ds_jobs_transformed['company_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_new_job:  ['1' '>4' 'never' '4' '3' '2' nan]\n",
      "last_new_job:  0            1\n",
      "1           >4\n",
      "2        never\n",
      "3        never\n",
      "4            4\n",
      "         ...  \n",
      "19153        1\n",
      "19154        4\n",
      "19155        4\n",
      "19156        2\n",
      "19157        1\n",
      "Name: last_new_job, Length: 19158, dtype: category\n",
      "Categories (6, object): ['never' < '1' < '2' < '3' < '4' < '>4']\n"
     ]
    }
   ],
   "source": [
    "# last_new_job\n",
    "print(\"last_new_job: \", ds_jobs_transformed['last_new_job'].unique())\n",
    "\n",
    "ls_last_new_job_order = ['never', '1', '2', '3', '4', '>4']\n",
    "\n",
    "ds_jobs_transformed['last_new_job'] = pd.Categorical(ds_jobs_transformed['last_new_job'], categories=ls_last_new_job_order, ordered=True)\n",
    "\n",
    "print(\"last_new_job: \", ds_jobs_transformed['last_new_job'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19158 entries, 0 to 19157\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count  Dtype   \n",
      "---  ------            --------------  -----   \n",
      " 0   city              19158 non-null  category\n",
      " 1   gender            14650 non-null  category\n",
      " 2   major_discipline  16345 non-null  category\n",
      " 3   company_type      13018 non-null  category\n",
      "dtypes: category(4)\n",
      "memory usage: 80.6 KB\n"
     ]
    }
   ],
   "source": [
    "# Nominal Data (converting to \"category\")\n",
    "\n",
    "ls_categorical_no_order = ['city', 'gender', 'major_discipline', 'company_type']\n",
    "\n",
    "for col in ls_categorical_no_order:\n",
    "    ds_jobs_transformed[col] = ds_jobs_transformed[col].astype('category')\n",
    "\n",
    "# Check the data types of the transformed dataset\n",
    "ds_jobs_transformed[ls_categorical_no_order].info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Goal\n",
    "\n",
    "This recruitment company wants to focus on:\n",
    "* more experienced professionals\n",
    "* enterprise companies\n",
    "\n",
    "Therefore, the DataFrame should be filtered to only contain:\n",
    "* 'experience' >= 10 year\n",
    "* 'company_size' >= 1000 employees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering dataset for business goals\n",
    "ds_jobs_transformed = ds_jobs_transformed[\n",
    "    (ds_jobs_transformed['experience'] >= '10') & \n",
    "    (ds_jobs_transformed['company_size'] >= '1000-4999')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Efficiency: Memory Usage (Old Dataframe vs. New Dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame memory usage: 10.51 MB\n",
      "Transformed DataFrame memory usage: 0.08 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original DataFrame memory usage: {ds_jobs.memory_usage(deep=True).sum() / 1024 ** 2:.2f} MB\")\n",
    "print(f\"Transformed DataFrame memory usage: {ds_jobs_transformed.memory_usage(deep=True).sum() / 1024 ** 2:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
